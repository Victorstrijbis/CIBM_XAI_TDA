import os

import numpy as np
import torch
import torch.nn as nn

import matplotlib.pyplot as plt

from blocks import ExtResNetBlock, create_encoders, \
    create_decoders
from data_loader import load_dataset, MDGunet
from utils import *
from losses import GeneralizedDiceLoss, FocalLoss
from metrics import DiceCoefficient

from tqdm import tqdm

def number_of_features_per_level(init_channel_number, num_levels):
    return [init_channel_number * 2 ** k for k in range(num_levels)]


def expand_as_one_hot(input, C, ignore_index=None):
    """
    Converts NxDxHxW label image to NxCxDxHxW, where each label gets converted to its corresponding one-hot vector
    :param input: 4D input image (NxDxHxW)
    :param C: number of channels/labels
    :param ignore_index: ignore index to be kept during the expansion
    :return: 5D output image (NxCxDxHxW)
    """
    
    # print("input shape:", input.shape)
    
    assert input.dim() == 4

    # expand the input tensor to Nx1xDxHxW before scattering
    input = input.unsqueeze(1)
    # create result tensor shape (NxCxDxHxW)
    shape = list(input.size())
    shape[1] = C

    if ignore_index is not None:
        # create ignore_index mask for the result
        mask = input.expand(shape) == ignore_index
        # clone the src tensor and zero out ignore_index in the input
        input = input.clone()
        input[input == ignore_index] = 0
        # scatter to get the one-hot tensor
        result = torch.zeros(shape).to(input.device).scatter_(1, input, 1)
        # bring back the ignore_index in the result
        result[mask] = ignore_index
        return result
    else:
        # scatter to get the one-hot tensor
        return torch.zeros(shape).to(input.device).scatter_(1, input, 1)

class Abstract3DUNet(nn.Module):
    """
    Base class for standard and residual UNet.
    Args:
        in_channels (int): number of input channels
        out_channels (int): number of output segmentation masks;
            Note that that the of out_channels might correspond to either
            different semantic classes or to different binary segmentation mask.
            It's up to the user of the class to interpret the out_channels and
            use the proper loss criterion during training (i.e. CrossEntropyLoss (multi-class)
            or BCEWithLogitsLoss (two-class) respectively)
        f_maps (int, tuple): number of feature maps at each level of the encoder; if it's an integer the number
            of feature maps is given by the geometric progression: f_maps ^ k, k=1,2,3,4
        final_sigmoid (bool): if True apply element-wise nn.Sigmoid after the
            final 1x1 convolution, otherwise apply nn.Softmax. MUST be True if nn.BCELoss (two-class) is used
            to train the model. MUST be False if nn.CrossEntropyLoss (multi-class) is used to train the model.
        basic_module: basic model for the encoder/decoder (DoubleConv, ExtResNetBlock, ....)
        layer_order (string): determines the order of layers
            in `SingleConv` module. e.g. 'crg' stands for Conv3d+ReLU+GroupNorm3d.
            See `SingleConv` for more info
        num_groups (int): number of groups for the GroupNorm
        num_levels (int): number of levels in the encoder/decoder path (applied only if f_maps is an int)
        is_segmentation (bool): if True (semantic segmentation problem) Sigmoid/Softmax normalization is applied
            after the final convolution; if False (regression problem) the normalization layer is skipped at the end
        testing (bool): if True (testing mode) the `final_activation` (if present, i.e. `is_segmentation=true`)
            will be applied as the last operation during the forward pass; if False the model is in training mode
            and the `final_activation` (even if present) won't be applied; default: False
        conv_kernel_size (int or tuple): size of the convolving kernel in the basic_module
        pool_kernel_size (int or tuple): the size of the window
        conv_padding (int or tuple): add zero-padding added to all three sides of the input
    """

    def __init__(self, in_channels, out_channels, final_sigmoid, basic_module, f_maps=64, layer_order='crg',
                 num_groups=8, num_levels=4, testing=False,
                 conv_kernel_size=3, pool_kernel_size=2, conv_padding=1, **kwargs):
        super(Abstract3DUNet, self).__init__()

        self.testing = testing

        if isinstance(f_maps, int):
            f_maps = number_of_features_per_level(f_maps, num_levels=num_levels)

        assert isinstance(f_maps, list) or isinstance(f_maps, tuple)
        assert len(f_maps) > 1, "Required at least 2 levels in the U-Net"

        # create encoder path
        self.encoders = create_encoders(in_channels, f_maps, basic_module, conv_kernel_size, conv_padding, layer_order,
                                        num_groups, pool_kernel_size)

        # create decoder path
        self.decoders = create_decoders(f_maps, basic_module, conv_kernel_size, conv_padding, layer_order, num_groups,
                                        upsample=True)

        # in the last layer a 1Ã—1 convolution reduces the number of output
        # channels to the number of labels
        self.final_conv = nn.Conv3d(f_maps[0], out_channels, 1)

        
        # semantic segmentation problem
        if final_sigmoid:
            self.final_activation = nn.Sigmoid()
        else:
            self.final_activation = nn.Softmax(dim=1)
        

    def forward(self, x):
        # encoder part
        encoders_features = []
        for encoder in self.encoders:
            x = encoder(x)
            # reverse the encoder outputs to be aligned with the decoder
            encoders_features.insert(0, x)

        # remove the last encoder's output from the list
        # !!remember: it's the 1st in the list
        encoders_features = encoders_features[1:]

        # decoder part
        for decoder, encoder_features in zip(self.decoders, encoders_features):
            # pass the output from the corresponding encoder and the output
            # of the previous decoder
            x = decoder(encoder_features, x)

        x = self.final_conv(x)

        # apply final_activation (i.e. Sigmoid or Softmax) only during prediction. During training the network outputs
        # logits and it's up to the user to normalize it before visualising with tensorboard or computing validation metric
        if self.testing and self.final_activation is not None:
            x = self.final_activation(x)

        return x



class ResidualUNet3D(Abstract3DUNet):
    """
    Residual 3DUnet model implementation based on https://arxiv.org/pdf/1706.00120.pdf.
    Uses ExtResNetBlock as a basic building block, summation joining instead
    of concatenation joining and transposed convolutions for upsampling (watch out for block artifacts).
    Since the model effectively becomes a residual net, in theory it allows for deeper UNet.
    """

    def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=64, layer_order='crg',
                 num_groups=8, num_levels=4, conv_padding=1, **kwargs):
        super(ResidualUNet3D, self).__init__(in_channels=in_channels,
                                             out_channels=out_channels,
                                             final_sigmoid=final_sigmoid,
                                             basic_module=ExtResNetBlock,
                                             f_maps=f_maps,
                                             layer_order=layer_order,
                                             num_groups=num_groups,
                                             num_levels=num_levels,
                                             conv_padding=conv_padding,
                                             **kwargs)
        
        self.dice_loss_func = GeneralizedDiceLoss()
        self.focal_loss_func = FocalLoss
        self.dice_metric = DiceCoefficient()

    def train_model(self,
                   config,
                   train_loader,
                   val_loader,
                   optimizer,
                   lr_scheduler,
                   start_epoch=0
                   ):
        
        for i in range(start_epoch, config['epochs']+1):
            # train epoch
            train_results = self.run_epoch(train_loader,
                                           optimizer,
                                           i,
                                           mode='train')
            
            # val epoch
            with torch.no_grad():
                val_results = self.run_epoch(val_loader,
                                             optimizer,
                                             i,
                                             mode='val')          
        
            lr_scheduler.step()
            
            # save model
            if i % config['epoch_save'] == 0:
                state_dict = self.state_dict()
                for key in state_dict.keys():
                    state_dict[key] = state_dict[key].cpu()
                    
                torch.save({
                        'epoch': i,
                        'state_dict' : state_dict,
                        'optimizer' : optimizer.state_dict(),
                        'lr_scheduler' : lr_scheduler.state_dict()},
                        os.path.join(config['out_dir'] + 'checkpoints/', '%03d.ckpt' % i))
   
            del train_results, val_results
            torch.cuda.empty_cache()
            
    
    def run_epoch(self,
                  data_loader,
                  optimizer,
                  epoch,
                  mode='train'):
        
        if mode == 'train':
            self.train()
        else:
            self.eval()
            
            
        if mode != 'test':
            losses, dices = [], []
            with tqdm(data_loader, unit='batch') as tepoch:
                for j, input_data in enumerate(tepoch):  
                    tepoch.set_description('%s %d'% (mode.capitalize(), epoch))
                    
                    # print(input_data)
                    # get data
                    x = input_data['image'].cuda()
                    y = input_data['label']     
                    
                    # print("x shape: ", x.shape)
                    
                    # print("y: ", y.shape)

                    # ignore organs with no ground truth label
                    truth_class_idxs = torch.unique(y).numpy().astype('int64')
                    if truth_class_idxs.shape[0] == 1:
                        continue
                    
                    # get one hot labels
                    # print("y: ", y.shape)
                    # print("y[:,0]: ", y[:, 0].shape)
                    
                    y_true = expand_as_one_hot(y[:, 0].long(),3).cuda()
                    # y_true = expand_as_one_hot(y.long(), 1)
                    y_true = y_true.cuda()
                    
                    # forward pass
                    y_pred = self.forward(x)
                    
                    # print(y_pred.shape)
                    # print(y_true.shape)
                    
                    # compute loss
                    dice_loss = self.dice_loss_func(y_pred, 
                                                    y_true[:, truth_class_idxs][:, 1:])
                    focal_loss = self.focal_loss_func(y_pred, 
                                                      y_true[:, truth_class_idxs][:, 1:],
                                                      alpha=0.5)
                    loss = dice_loss + focal_loss
                    losses.append(loss.item())
                    
                    # backward pass
                    if mode == 'train':
                        optimizer.zero_grad()
                        loss.backward()                    
                        nn.utils.clip_grad_norm_(self.parameters(), max_norm=4.0, norm_type=2)
                        optimizer.step()
                        
                    # compute metric
                    mean_dice = self.dice_metric(torch.sigmoid(
                                                 y_pred), 
                                                 y_true[:, truth_class_idxs][:, 1:])
                    dices.append(mean_dice.item())
                    
                    tepoch.set_postfix(loss = np.average(losses),
                                       dice = np.average(dices))

                    del x, y_true, y_pred
                    torch.cuda.empty_cache()
                    
    
            return 0
        else:
            losses, dices, y_preds, y_trues, xs = [], [], [], [], []
            with tqdm(data_loader, unit='batch') as tepoch:
                for j, input_data in enumerate(tepoch):  
                    tepoch.set_description('%s %d'% (mode.capitalize(), epoch))
                    # get data
                    x = input_data['image'].cuda()
                    y = input_data['label']           

                    # print("x shape: ", x.shape)
                    # ignore organs with no ground truth label
                    truth_class_idxs = torch.unique(y).numpy().astype('int64')
                    if truth_class_idxs.shape[0] == 1:
                        continue
                    
                    # get one hot labels
                    # print(y[:, 0])
                    # y_true = expand_as_one_hot(y[:, 0].long())
                    y_true = expand_as_one_hot(y[:, 0].long(), 3)
                    y_true = y_true.cuda()
                    
                    # forward pass
                    y_pred = torch.sigmoid(self.forward(x))
                    
                    # print("y_pred: ", y_pred.shape)
                    # print("y_true: ", y_true.shape)
                    # compute metric
                    dsc = self.dice_metric(y_pred,y_true[:, truth_class_idxs][:, 1:])
                    
                    dices.append(dsc.item())
                    
                    tepoch.set_postfix(loss = np.average(losses),
                                       dice = np.average(dices))
                    
                    y_preds.append(y_pred)
                    y_trues.append(y_true)
                    xs.append(x)
                    

                    
                    del x, y_true, y_pred
                    
                    # print("HelloWorld")
                    torch.cuda.empty_cache()
            
            
            return y_preds, y_trues, dices, xs
        
        
# class ResidualUNet2D(Abstract2DUNet):
#     """
#     Residual 2DUnet model implementation based on https://arxiv.org/pdf/1706.00120.pdf.
#     Uses ExtResNetBlock as a basic building block, summation joining instead
#     of concatenation joining and transposed convolutions for upsampling (watch out for block artifacts).
#     Since the model effectively becomes a residual net, in theory it allows for deeper UNet.
#     """

#     def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=64, layer_order='crg',
#                  num_groups=8, num_levels=4, conv_padding=1, **kwargs):
#         super(ResidualUNet2D, self).__init__(in_channels=in_channels,
#                                              out_channels=out_channels,
#                                              final_sigmoid=final_sigmoid,
#                                              basic_module=ExtResNetBlock,
#                                              f_maps=f_maps,
#                                              layer_order=layer_order,
#                                              num_groups=num_groups,
#                                              num_levels=num_levels,
#                                              conv_padding=conv_padding,
#                                              **kwargs)
        
#         self.dice_loss_func = GeneralizedDiceLoss()
#         self.focal_loss_func = FocalLoss
#         self.dice_metric = DiceCoefficient()

#     def train_model(self,
#                    config,
#                    train_loader,
#                    val_loader,
#                    optimizer,
#                    lr_scheduler,
#                    start_epoch=0
#                    ):
        
#         for i in range(start_epoch, config['epochs']+1):
#             # train epoch
#             train_results = self.run_epoch(train_loader,
#                                            optimizer,
#                                            i,
#                                            mode='train')
            
#             # val epoch
#             with torch.no_grad():
#                 val_results = self.run_epoch(val_loader,
#                                              optimizer,
#                                              i,
#                                              mode='val')          
        
#             lr_scheduler.step()
            
#             # save model
#             if i % config['epoch_save'] == 0:
#                 state_dict = self.state_dict()
#                 for key in state_dict.keys():
#                     state_dict[key] = state_dict[key].cpu()
                    
#                 torch.save({
#                         'epoch': i,
#                         'state_dict' : state_dict,
#                         'optimizer' : optimizer.state_dict(),
#                         'lr_scheduler' : lr_scheduler.state_dict()},
#                         os.path.join(config['out_dir'] + 'checkpoints/', '%03d.ckpt' % i))
   
#             del train_results, val_results
#             torch.cuda.empty_cache()
            
    
#     def run_epoch(self,
#                   data_loader,
#                   optimizer,
#                   epoch,
#                   mode='train'):
        
#         if mode == 'train':
#             self.train()
#         else:
#             self.eval()
            
            
#         if mode != 'test':
#             losses, dices = [], []
#             with tqdm(data_loader, unit='batch') as tepoch:
#                 for j, input_data in enumerate(tepoch):  
#                     tepoch.set_description('%s %d'% (mode.capitalize(), epoch))
#                     # get data
#                     x = input_data['image'].cuda()
#                     y = input_data['label']           

#                     # ignore organs with no ground truth label
#                     truth_class_idxs = torch.unique(y).numpy().astype('int64')
#                     if truth_class_idxs.shape[0] == 1:
#                         continue
                    
#                     # get one hot labels
#                     # print(y[:, 0])
#                     # y_true = expand_as_one_hot(y[:, 0].long())
#                     y_true = expand_as_one_hot(y[:, 0].long(), 3)
#                     y_true = y_true.cuda()
                    
#                     # forward pass
#                     y_pred = self.forward(x)
                    
#                     # compute loss
#                     dice_loss = self.dice_loss_func(y_pred[:, truth_class_idxs][:, 1:], 
#                                                     y_true[:, truth_class_idxs][:, 1:])
#                     focal_loss = self.focal_loss_func(y_pred[:, truth_class_idxs][:, 1:], 
#                                                       y_true[:, truth_class_idxs][:, 1:],
#                                                       alpha=0.5)
#                     loss = dice_loss + focal_loss
#                     losses.append(loss.item())
                    
#                     # backward pass
#                     if mode == 'train':
#                         optimizer.zero_grad()
#                         loss.backward()                    
#                         nn.utils.clip_grad_norm_(self.parameters(), max_norm=4.0, norm_type=2)
#                         optimizer.step()
                        
#                     # compute metric
#                     mean_dice = self.dice_metric(torch.sigmoid(
#                                                  y_pred[:, truth_class_idxs][:, 1:]), 
#                                                  y_true[:, truth_class_idxs][:, 1:])
#                     dices.append(mean_dice.item())
                    
#                     tepoch.set_postfix(loss = np.average(losses),
#                                        dice = np.average(dices))

#                     del x, y_true, y_pred
#                     torch.cuda.empty_cache()
                    
    
#             return 0
#         else:
#             losses, dices, y_preds, y_trues, xs = [], [], [], [], []
#             with tqdm(data_loader, unit='batch') as tepoch:
#                 for j, input_data in enumerate(tepoch):  
#                     tepoch.set_description('%s %d'% (mode.capitalize(), epoch))
#                     # get data
#                     x = input_data['image'].cuda()
#                     y = input_data['label']           

                    
#                     # ignore organs with no ground truth label
#                     truth_class_idxs = torch.unique(y).numpy().astype('int64')
#                     if truth_class_idxs.shape[0] == 1:
#                         continue
                    
#                     # get one hot labels
#                     # print(y[:, 0])
#                     # y_true = expand_as_one_hot(y[:, 0].long())
#                     y_true = expand_as_one_hot(y[:, 0].long(), 3)
#                     y_true = y_true.cuda()
                    
#                     # forward pass
#                     y_pred = torch.sigmoid(self.forward(x)[:,truth_class_idxs][:,1:])
                    
#                     # compute metric
#                     dsc = self.dice_metric(y_pred,y_true[:, truth_class_idxs][:, 1:])
                    
#                     dices.append(dsc.item())
                    
#                     tepoch.set_postfix(loss = np.average(losses),
#                                        dice = np.average(dices))
                    
#                     y_preds.append(y_pred)
#                     y_trues.append(y_true)
#                     xs.append(x)
                    

                    
#                     del x, y_true, y_pred
#                     torch.cuda.empty_cache()
            
            
#             return y_preds, y_trues, dices, xs
